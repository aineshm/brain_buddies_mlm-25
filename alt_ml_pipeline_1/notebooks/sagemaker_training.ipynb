{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alt ML Pipeline 1 - SageMaker Training Notebook\n",
    "\n",
    "This notebook trains the YOLO model on AWS SageMaker with GPU acceleration.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. Upload your data to S3:\n",
    "   ```bash\n",
    "   aws s3 sync ml_pipeline/data/processed/yolo_dataset/ s3://your-bucket/alt-pipeline-1/yolo_dataset/\n",
    "   ```\n",
    "\n",
    "2. Set your S3 bucket name in the cell below\n",
    "\n",
    "3. Ensure you're running this in a SageMaker notebook instance or SageMaker Studio\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Downloads data from S3\n",
    "2. Prepares cross-validation splits\n",
    "3. Trains YOLO model with GPU\n",
    "4. Evaluates performance\n",
    "5. Saves model and metrics\n",
    "6. Uploads results back to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================================================\n",
    "\n",
    "# Your S3 bucket name (REQUIRED - update this!)\n",
    "S3_BUCKET = \"your-bucket-name\"  # ← CHANGE THIS\n",
    "\n",
    "# S3 paths\n",
    "S3_DATA_PREFIX = \"alt-pipeline-1/yolo_dataset\"  # Where you uploaded the data\n",
    "S3_OUTPUT_PREFIX = \"alt-pipeline-1/training-output\"  # Where to save results\n",
    "\n",
    "# Training configuration\n",
    "FOLD_IDX = 0  # Which fold to train (0-4)\n",
    "NUM_EPOCHS = 100  # Number of training epochs\n",
    "BATCH_SIZE = 16  # Batch size (can be larger on GPU)\n",
    "IMAGE_SIZE = 640  # Input image size\n",
    "\n",
    "# Model selection\n",
    "MODEL_NAME = \"yolov8n-seg.pt\"  # Options: yolov8n-seg, yolov8s-seg, yolov8m-seg\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = [\n",
    "    \"planktonic\",\n",
    "    \"single_dispersed\",\n",
    "    \"hyphae\",\n",
    "    \"clump_dispersed\",\n",
    "    \"yeast\",\n",
    "    \"biofilm\",\n",
    "    \"pseudohyphae\"\n",
    "]\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"  Training Fold: {FOLD_IDX}\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# ============================================================================\n",
    "\n",
    "# Install ultralytics and other dependencies if not already installed\n",
    "!pip install -q ultralytics==8.0.227 opencv-python-headless PyYAML tqdm\n",
    "\n",
    "print(\"✓ Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable WandB and MLflow integrations (we'll track manually)\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "from ultralytics import YOLO, settings\n",
    "import torch\n",
    "\n",
    "# Disable Ultralytics integrations\n",
    "settings['wandb'] = False\n",
    "settings['mlflow'] = False\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD DATA FROM S3\n",
    "# ============================================================================\n",
    "\n",
    "# Create local data directory\n",
    "LOCAL_DATA_DIR = Path(\"/tmp/yolo_dataset\")\n",
    "LOCAL_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Downloading data from S3...\")\n",
    "print(f\"  Source: s3://{S3_BUCKET}/{S3_DATA_PREFIX}/\")\n",
    "print(f\"  Destination: {LOCAL_DATA_DIR}\")\n",
    "print()\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List all objects in the S3 prefix\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_DATA_PREFIX)\n",
    "\n",
    "files_to_download = []\n",
    "for page in pages:\n",
    "    if 'Contents' in page:\n",
    "        for obj in page['Contents']:\n",
    "            files_to_download.append(obj['Key'])\n",
    "\n",
    "print(f\"Found {len(files_to_download)} files to download\")\n",
    "print()\n",
    "\n",
    "# Download files with progress bar\n",
    "for s3_key in tqdm(files_to_download, desc=\"Downloading\"):\n",
    "    # Remove prefix to get relative path\n",
    "    rel_path = s3_key.replace(S3_DATA_PREFIX + '/', '')\n",
    "    if not rel_path:  # Skip if it's the prefix itself\n",
    "        continue\n",
    "    \n",
    "    local_file = LOCAL_DATA_DIR / rel_path\n",
    "    local_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download file\n",
    "    s3.download_file(S3_BUCKET, s3_key, str(local_file))\n",
    "\n",
    "print(\"\\n✓ Data downloaded successfully\")\n",
    "\n",
    "# Verify directory structure\n",
    "train_images = list((LOCAL_DATA_DIR / 'images' / 'train').glob('*.tif*'))\n",
    "train_labels = list((LOCAL_DATA_DIR / 'labels' / 'train').glob('*.txt'))\n",
    "val_images = list((LOCAL_DATA_DIR / 'images' / 'val').glob('*.tif*'))\n",
    "val_labels = list((LOCAL_DATA_DIR / 'labels' / 'val').glob('*.txt'))\n",
    "\n",
    "print(f\"\\nData verification:\")\n",
    "print(f\"  Training images: {len(train_images)}\")\n",
    "print(f\"  Training labels: {len(train_labels)}\")\n",
    "print(f\"  Validation images: {len(val_images)}\")\n",
    "print(f\"  Validation labels: {len(val_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYZE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Count annotations per class\n",
    "class_counts = Counter()\n",
    "total_annotations = 0\n",
    "\n",
    "for label_file in train_labels + val_labels:\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 0:\n",
    "                class_id = int(parts[0])\n",
    "                class_counts[class_id] += 1\n",
    "                total_annotations += 1\n",
    "\n",
    "print(f\"Total images: {len(train_images) + len(val_images)}\")\n",
    "print(f\"  Training: {len(train_images)}\")\n",
    "print(f\"  Validation: {len(val_images)}\")\n",
    "print()\n",
    "print(f\"Total annotations: {total_annotations}\")\n",
    "print()\n",
    "print(\"Class distribution:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    count = class_counts.get(i, 0)\n",
    "    percentage = (count / total_annotations * 100) if total_annotations > 0 else 0\n",
    "    print(f\"  {i}. {class_name:20s}: {count:5d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE CROSS-VALIDATION SPLITS (Leave-One-Sequence-Out)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating cross-validation splits...\")\n",
    "print()\n",
    "\n",
    "# Group images by sequence name\n",
    "sequences = {}\n",
    "all_images = train_images + val_images\n",
    "\n",
    "for img_path in all_images:\n",
    "    # Extract sequence name (e.g., \"MattLines1\" from \"MattLines1_frame_0000.tif\")\n",
    "    filename = img_path.stem\n",
    "    parts = filename.split('_frame_')\n",
    "    if len(parts) == 2:\n",
    "        seq_name = parts[0]\n",
    "        if seq_name not in sequences:\n",
    "            sequences[seq_name] = []\n",
    "        sequences[seq_name].append(img_path)\n",
    "\n",
    "print(f\"Found {len(sequences)} sequences:\")\n",
    "for seq_name, images in sequences.items():\n",
    "    print(f\"  {seq_name}: {len(images)} frames\")\n",
    "print()\n",
    "\n",
    "# Create splits\n",
    "sequence_names = list(sequences.keys())\n",
    "splits = []\n",
    "\n",
    "for i, val_seq in enumerate(sequence_names):\n",
    "    split = {\n",
    "        'fold': i,\n",
    "        'val_sequence': val_seq,\n",
    "        'train_images': [],\n",
    "        'val_images': sequences[val_seq]\n",
    "    }\n",
    "    \n",
    "    # Add all other sequences to training\n",
    "    for seq_name in sequence_names:\n",
    "        if seq_name != val_seq:\n",
    "            split['train_images'].extend(sequences[seq_name])\n",
    "    \n",
    "    splits.append(split)\n",
    "\n",
    "print(f\"Created {len(splits)} folds:\")\n",
    "for split in splits:\n",
    "    print(f\"  Fold {split['fold']}: Val={split['val_sequence']}, \"\n",
    "          f\"Train={len(split['train_images'])}, Val={len(split['val_images'])}\")\n",
    "\n",
    "print(f\"\\n✓ Splits created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE SELECTED FOLD\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nPreparing fold {FOLD_IDX}...\")\n",
    "print()\n",
    "\n",
    "split = splits[FOLD_IDX]\n",
    "print(f\"Validation sequence: {split['val_sequence']}\")\n",
    "print(f\"Training images: {len(split['train_images'])}\")\n",
    "print(f\"Validation images: {len(split['val_images'])}\")\n",
    "print()\n",
    "\n",
    "# Create fold directory\n",
    "FOLD_DIR = Path(f\"/tmp/fold_{FOLD_IDX}\")\n",
    "FOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create subdirectories\n",
    "(FOLD_DIR / 'images' / 'train').mkdir(parents=True, exist_ok=True)\n",
    "(FOLD_DIR / 'images' / 'val').mkdir(parents=True, exist_ok=True)\n",
    "(FOLD_DIR / 'labels' / 'train').mkdir(parents=True, exist_ok=True)\n",
    "(FOLD_DIR / 'labels' / 'val').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy training images and labels\n",
    "print(\"Copying training data...\")\n",
    "for img_path in tqdm(split['train_images'], desc=\"Train\"):\n",
    "    # Copy image\n",
    "    dest_img = FOLD_DIR / 'images' / 'train' / img_path.name\n",
    "    if not dest_img.exists():\n",
    "        shutil.copy(img_path, dest_img)\n",
    "    \n",
    "    # Copy label\n",
    "    label_name = img_path.stem + '.txt'\n",
    "    src_label = LOCAL_DATA_DIR / 'labels' / 'train' / label_name\n",
    "    if not src_label.exists():\n",
    "        src_label = LOCAL_DATA_DIR / 'labels' / 'val' / label_name\n",
    "    \n",
    "    if src_label.exists():\n",
    "        dest_label = FOLD_DIR / 'labels' / 'train' / label_name\n",
    "        if not dest_label.exists():\n",
    "            shutil.copy(src_label, dest_label)\n",
    "\n",
    "# Copy validation images and labels\n",
    "print(\"Copying validation data...\")\n",
    "for img_path in tqdm(split['val_images'], desc=\"Val\"):\n",
    "    # Copy image\n",
    "    dest_img = FOLD_DIR / 'images' / 'val' / img_path.name\n",
    "    if not dest_img.exists():\n",
    "        shutil.copy(img_path, dest_img)\n",
    "    \n",
    "    # Copy label\n",
    "    label_name = img_path.stem + '.txt'\n",
    "    src_label = LOCAL_DATA_DIR / 'labels' / 'train' / label_name\n",
    "    if not src_label.exists():\n",
    "        src_label = LOCAL_DATA_DIR / 'labels' / 'val' / label_name\n",
    "    \n",
    "    if src_label.exists():\n",
    "        dest_label = FOLD_DIR / 'labels' / 'val' / label_name\n",
    "        if not dest_label.exists():\n",
    "            shutil.copy(src_label, dest_label)\n",
    "\n",
    "# Create dataset YAML file\n",
    "dataset_yaml = {\n",
    "    'path': str(FOLD_DIR.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc': len(CLASS_NAMES),\n",
    "    'names': CLASS_NAMES\n",
    "}\n",
    "\n",
    "yaml_path = FOLD_DIR / 'data.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_yaml, f)\n",
    "\n",
    "print(f\"\\n✓ Fold {FOLD_IDX} prepared at: {FOLD_DIR}\")\n",
    "print(f\"  Dataset YAML: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN YOLO MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"TRAINING YOLO MODEL - Fold {FOLD_IDX}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Image size: {IMAGE_SIZE}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print()\n",
    "\n",
    "# Initialize model\n",
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    'data': str(yaml_path),\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'imgsz': IMAGE_SIZE,\n",
    "    'batch': BATCH_SIZE,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
    "    'patience': 20,  # Early stopping patience\n",
    "    'save': True,\n",
    "    'project': '/tmp/training_output',\n",
    "    'name': f'fold_{FOLD_IDX}',\n",
    "    'exist_ok': True,\n",
    "    'pretrained': True,\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr0': 0.001,\n",
    "    'weight_decay': 0.0005,\n",
    "    'plots': True,\n",
    "    'save_period': 10,  # Save checkpoint every 10 epochs\n",
    "    'val': True,\n",
    "    'workers': 8\n",
    "}\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "results = model.train(**train_params)\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"✓ Training complete!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE MODEL ON VALIDATION SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Evaluating model on validation set...\")\n",
    "print()\n",
    "\n",
    "# Run validation\n",
    "metrics = model.val()\n",
    "\n",
    "# Extract metrics\n",
    "results_dict = {\n",
    "    'fold': FOLD_IDX,\n",
    "    'validation_sequence': split['val_sequence'],\n",
    "    'mAP50': float(metrics.box.map50) if hasattr(metrics, 'box') else 0.0,\n",
    "    'mAP50-95': float(metrics.box.map) if hasattr(metrics, 'box') else 0.0,\n",
    "    'precision': float(metrics.box.mp) if hasattr(metrics, 'box') else 0.0,\n",
    "    'recall': float(metrics.box.mr) if hasattr(metrics, 'box') else 0.0,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Calculate F1 score\n",
    "if results_dict['precision'] + results_dict['recall'] > 0:\n",
    "    results_dict['f1'] = 2 * (results_dict['precision'] * results_dict['recall']) / \\\n",
    "                         (results_dict['precision'] + results_dict['recall'])\n",
    "else:\n",
    "    results_dict['f1'] = 0.0\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFold {FOLD_IDX} - Validation sequence: {split['val_sequence']}\")\n",
    "print()\n",
    "for key, value in results_dict.items():\n",
    "    if key not in ['fold', 'validation_sequence', 'timestamp']:\n",
    "        print(f\"  {key:15s}: {value:.4f}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_file = Path('/tmp/training_output') / f'fold_{FOLD_IDX}' / 'metrics.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metrics saved to: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UPLOAD RESULTS TO S3\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Uploading results to S3...\")\n",
    "print()\n",
    "\n",
    "# Paths\n",
    "training_output_dir = Path('/tmp/training_output') / f'fold_{FOLD_IDX}'\n",
    "best_model_path = training_output_dir / 'weights' / 'best.pt'\n",
    "last_model_path = training_output_dir / 'weights' / 'last.pt'\n",
    "\n",
    "# S3 prefix for this training run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "s3_output_prefix = f\"{S3_OUTPUT_PREFIX}/fold_{FOLD_IDX}/{timestamp}\"\n",
    "\n",
    "# Files to upload\n",
    "files_to_upload = [\n",
    "    (metrics_file, f\"{s3_output_prefix}/metrics.json\"),\n",
    "    (best_model_path, f\"{s3_output_prefix}/best.pt\"),\n",
    "    (last_model_path, f\"{s3_output_prefix}/last.pt\"),\n",
    "]\n",
    "\n",
    "# Upload files\n",
    "for local_path, s3_key in files_to_upload:\n",
    "    if local_path.exists():\n",
    "        print(f\"  Uploading {local_path.name} → s3://{S3_BUCKET}/{s3_key}\")\n",
    "        s3.upload_file(str(local_path), S3_BUCKET, s3_key)\n",
    "    else:\n",
    "        print(f\"  ⚠ Skipping {local_path.name} (not found)\")\n",
    "\n",
    "# Upload training plots if they exist\n",
    "plots_dir = training_output_dir\n",
    "for plot_file in plots_dir.glob('*.png'):\n",
    "    s3_key = f\"{s3_output_prefix}/plots/{plot_file.name}\"\n",
    "    print(f\"  Uploading {plot_file.name} → s3://{S3_BUCKET}/{s3_key}\")\n",
    "    s3.upload_file(str(plot_file), S3_BUCKET, s3_key)\n",
    "\n",
    "print()\n",
    "print(\"✓ Results uploaded to S3\")\n",
    "print(f\"  Location: s3://{S3_BUCKET}/{s3_output_prefix}/\")\n",
    "print()\n",
    "print(\"To download results:\")\n",
    "print(f\"  aws s3 sync s3://{S3_BUCKET}/{s3_output_prefix}/ ./local_results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Fold trained: {FOLD_IDX}\")\n",
    "print(f\"Validation sequence: {split['val_sequence']}\")\n",
    "print()\n",
    "print(\"Performance metrics:\")\n",
    "print(f\"  F1 Score:   {results_dict['f1']:.4f}\")\n",
    "print(f\"  mAP50:      {results_dict['mAP50']:.4f}\")\n",
    "print(f\"  Precision:  {results_dict['precision']:.4f}\")\n",
    "print(f\"  Recall:     {results_dict['recall']:.4f}\")\n",
    "print()\n",
    "print(\"Model saved to:\")\n",
    "print(f\"  Best:  s3://{S3_BUCKET}/{s3_output_prefix}/best.pt\")\n",
    "print(f\"  Last:  s3://{S3_BUCKET}/{s3_output_prefix}/last.pt\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(f\"  1. Review metrics above\")\n",
    "print(f\"  2. Train other folds (change FOLD_IDX and rerun)\")\n",
    "print(f\"  3. Download models from S3 for inference\")\n",
    "print(f\"  4. Compare performance across folds\")\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
